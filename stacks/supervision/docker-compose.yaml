version: "3.8"

volumes:
  prometheus-data:
  prometheus-config:
  grafana-data:
  grafana-config:
  uptime_kuma-data:
  loki-config:
  loki-data:


x-logging: &default-logging
  driver: "json-file"
  options:
    tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"

services:
  grafana:
    image: grafana/grafana:11.2.0
    container_name: grafana
    logging: *default-logging
    restart: always
    volumes:
      - grafana-data:/var/lib/grafana/
      - grafana-config:/etc/grafana/
    env_file:
      - stack.env
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /etc/grafana/provisioning/datasources
        cat <<EOF > /etc/grafana/provisioning/datasources/ds.yaml
        apiVersion: 1
        datasources:
        - name: Loki
          type: loki
          access: proxy
          orgId: 1
          url: http://loki:3100
          basicAuth: false
          version: 1
          editable: false
        - name: Prometheus
          type: prometheus
          access: proxy
          isDefault: true
          url: http://prometheus:9090
          jsonData:
            httpMethod: POST
            manageAlerts: true
            prometheusType: Prometheus
            prometheusVersion: 2.27.0
            cacheLevel: 'High'
            disableRecordingRules: false
            incrementalQueryOverlapWindow: 10m
        EOF
        /run.sh
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 500M
        reservations:
          cpus: '0.1'
          memory: 100M

  loki:
    image: grafana/loki:3.0.0
    container_name: loki
    logging: *default-logging
    restart: always
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki-data:/loki
      - loki-config:/etc/loki/
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 800M
        reservations:
          cpus: '0.2'
          memory: 150M

  prometheus:
    image: prom/prometheus:v2.27.0
    container_name: prometheus
    logging: *default-logging
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - prometheus-config:/etc/prometheus/
    command: [ '--config.file=/etc/prometheus/prometheus.yml', '--storage.tsdb.retention.time=90d' ]
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 800M
        reservations:
          cpus: '0.2'
          memory: 150M

  blackbox_exporter:
    image: quay.io/prometheus/blackbox-exporter:latest
    container_name: blackbox_exporter
    logging: *default-logging
    restart: always
    #    ports:
    #      - "9115:9115"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 150M
        reservations:
          cpus: '0.1'
          memory: 20M

  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: uptime-kuma
    logging: *default-logging
    restart: always
    ports:
      - 3001:3001
    volumes:
      - uptime_kuma-data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      resources:
        limits:
          cpus: '0.8'
          memory: 900M
        reservations:
          cpus: '0.5'
          memory: 500M

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    logging: *default-logging
    restart: always
    # ports:
    # - 8091:8080
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 150M
        reservations:
          cpus: '0.1'
          memory: 20M

  legrand_exporter:
    image: apresse/legrand_exporter:0.8
    container_name: legrand_exporter
    logging: *default-logging
    restart: always
    env_file:
      - stack.env
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 150M
        reservations:
          cpus: '0.1'
          memory: 20M

  meross_exporter:
    image: apresse/meross_exporter:0.2
    container_name: meross_exporter
    logging: *default-logging
    restart: no
    env_file:
      - stack.env
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 150M
        reservations:
          cpus: '0.1'
          memory: 20M

  meteo_exporter:
    image: apresse/meteofrance_exporter:0.1
    container_name: meteo_exporter
    logging: *default-logging
    restart: always
    env_file:
      - stack.env
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 150M
        reservations:
          cpus: '0.1'
          memory: 20M
  pal_exporter:
    image: docker.io/bostrt/palworld-exporter:latest
    restart: always
    container_name: palworld_exporter
    logging: *default-logging
    ports:
      - 9877:9877/tcp
    env_file:
      - stack.env
    environment:
      - RCON_HOST=185.216.27.61
      - RCON_PORT=25575

  grafana-to-ntfy:
    container_name: grafana-to-ntfy
    image: saibe1111/grafana-to-ntfy
    restart: always
    env_file:
      - stack.env

  qbittorrent_exporter:
    container_name: qbittorrent_exporter
    image: ghcr.io/esanchezm/prometheus-qbittorrent-exporter
    restart: always
    env_file:
      - stack.env
